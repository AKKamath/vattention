[
    {
      "directory": "/home/t-adikamath/sangam-fused-attention/fused_ampere/",
      "command": "clang++ --gcc-install-dir=/home/t-adikamath/miniconda3/lib/gcc/x86_64-conda-linux-gnu/11.2.0 --cuda-gpu-arch=sm_80 --cuda-device-only --cuda-path=/usr/local/cuda \
          -resource-dir /home/t-adikamath/miniconda3/lib/clang/16 -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ \
          -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ \
          -DCUTLASS_DEBUG_TRACE_LEVEL=0 -DNDEBUG -c flash_fwd_hdim128_fp16_sm80.cu \
          -I/home/t-adikamath/sangam-fused-attention/csrc/cutlass/include \
          -I/home/t-adikamath/miniconda3/lib/python3.12/site-packages/torch/include \
          -I/home/t-adikamath/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include \
          -I/home/t-adikamath/miniconda3/lib/python3.12/site-packages/torch/include/TH \
          -I/home/t-adikamath/miniconda3/lib/python3.12/site-packages/torch/include/THC \
          -I/home/t-adikamath/miniconda3/include -I/home/t-adikamath/miniconda3/include/python3.12 \
           -Xclang -fcuda-allow-variadic-functions",
      "file": "/fused_ampere/flash_fwd_hdim128_fp16_sm80.cu"
    }
  ]