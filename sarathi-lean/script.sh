python sarathi/benchmark/main.py --model_name meta-llama/Meta-Llama-3-8B --model_tensor_parallel_degree 2 --request_generator_provider synthetic --synthetic_request_generator_length_provider trace --synthetic_request_generator_interval_provider poisson --poisson_request_interval_generator_qps 7 --trace_request_length_generator_trace_file ../../decode_tpt_experiments/vllm/data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv --replica_scheduler_provider sarathi --sarathi_scheduler_chunk_size 2048 --trace_request_length_generator_prefill_scale_factor 1 --trace_request_length_generator_decode_scale_factor 1 --replica_scheduler_max_batch_size 300 --vllm_scheduler_max_tokens_in_batch 20480 --model_max_model_len 4096 --metrics_store_enable_op_level_metrics false --metrics_store_keep_individual_batch_metrics true --output_dir ./check/fa_vattn/7_qps --synthetic_request_generator_num_requests 512 --trace_request_generator_max_tokens 20480 --model_block_size 32 --model_attention_backend VATTN_FA --metrics_store_enable_op_level_metrics true --metrics_store_enable_cpu_op_level_metrics true> compile

python sarathi/benchmark/main.py --model_name meta-llama/Meta-Llama-3-8B --model_tensor_parallel_degree 2 --request_generator_provider synthetic --synthetic_request_generator_length_provider trace --synthetic_request_generator_interval_provider poisson --poisson_request_interval_generator_qps 7 --trace_request_length_generator_trace_file ../../decode_tpt_experiments/vllm/data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv --replica_scheduler_provider sarathi --sarathi_scheduler_chunk_size 2048 --trace_request_length_generator_prefill_scale_factor 1 --trace_request_length_generator_decode_scale_factor 1 --replica_scheduler_max_batch_size 300 --vllm_scheduler_max_tokens_in_batch 20480 --model_max_model_len 20480 --metrics_store_enable_op_level_metrics false --metrics_store_keep_individual_batch_metrics true --output_dir ./check/fa/7_qps --synthetic_request_generator_num_requests 512 --trace_request_generator_max_tokens 20480 --model_block_size 32 --model_attention_backend FLASH_ATTENTION  --metrics_store_enable_op_level_metrics true --metrics_store_enable_cpu_op_level_metrics true> compile
